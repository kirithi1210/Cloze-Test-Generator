from __future__ import annotations
import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List, Sequence, Tuple

import torch
import spacy
from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline

try:
    import gradio as gr
except ImportError:
    gr = None


class ClozeGenerator:
    def __init__(self, model_name: str = "bert-base-uncased", nlp_model: str = "en_core_web_sm", device: int = -1):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForMaskedLM.from_pretrained(model_name)
        self.fill_mask = pipeline("fill-mask", model=self.model, tokenizer=self.tokenizer, device=device)

        try:
            self.nlp = spacy.load(nlp_model)
        except OSError:
            import subprocess
            subprocess.run([sys.executable, "-m", "spacy", "download", nlp_model], check=True)
            self.nlp = spacy.load(nlp_model)

    def generate(
        self,
        text: str,
        num_blanks: int = 5,
        difficulty: str = "medium",
        pos_targets: Sequence[str] | None = None,
        avoid_ner: bool = True,
    ) -> Tuple[str, Dict[str, str]]:
        doc = self.nlp(text)
        candidates = self._select_tokens(doc, pos_targets, avoid_ner)
        if not candidates:
            raise ValueError("No suitable tokens to blank out.")
        if num_blanks > len(candidates):
            num_blanks = len(candidates)
        scored = self._score_tokens(text, candidates)
        if difficulty == "easy":
            scored.sort(key=lambda s: s.score, reverse=True)
        elif difficulty == "hard":
            scored.sort(key=lambda s: s.score)
        else:
            scored.sort(key=lambda s: abs(0.5 - s.score))
        chosen = {s.token.i for s in scored[:num_blanks]}
        return self._build_output(doc, chosen)

    class _Scored:
        def __init__(self, token: spacy.tokens.Token, score: float):
            self.token = token
            self.score = score

    def _select_tokens(self, doc: spacy.tokens.Doc, pos_targets: Sequence[str] | None, avoid_ner: bool):
        pos_targets = {p.lower() for p in pos_targets} if pos_targets else {"nouns", "verbs", "adjectives"}
        pos_map = {"NOUN": "nouns", "PROPN": "nouns", "VERB": "verbs", "ADJ": "adjectives"}

        def valid(tok: spacy.tokens.Token) -> bool:
            if not tok.is_alpha or tok.is_stop:
                return False
            if pos_map.get(tok.pos_) not in pos_targets:
                return False
            if avoid_ner and tok.ent_type_:
                return False
            return len(self.tokenizer.tokenize(tok.text)) == 1

        return [tok for tok in doc if valid(tok)]

    def _score_tokens(self, text: str, toks: List[spacy.tokens.Token], top_k: int = 5):
        mask = self.tokenizer.mask_token
        scored = []
        for tok in toks:
            masked = text[: tok.idx] + mask + text[tok.idx + len(tok.text):]
            try:
                preds = self.fill_mask(masked, top_k=top_k)
            except RuntimeError:
                preds = []
            score = 0.0
            for p in preds:
                if p["token_str"].strip().lower() == tok.text.lower():
                    score = p["score"]
                    break
            scored.append(self._Scored(tok, score))
        return scored

    def _build_output(self, doc: spacy.tokens.Doc, blank_ids: set[int]) -> Tuple[str, Dict[str, str]]:
        parts = []
        answers = {}
        idx = 1
        for tok in doc:
            if tok.i in blank_ids:
                parts.append(f"___({idx})___")
                answers[str(idx)] = tok.text
                idx += 1
            else:
                parts.append(tok.text)
            parts.append(tok.whitespace_)
        return "".join(parts).strip(), answers

    @staticmethod
    def export_to_json(path: Path, quiz: str, ans: Dict[str, str]) -> None:
        with open(path, "w", encoding="utf-8") as f:
            json.dump({"quiz": quiz, "answers": ans}, f, indent=2, ensure_ascii=False)

    @staticmethod
    def build_arg_parser() -> argparse.ArgumentParser:
        p = argparse.ArgumentParser(description="Generate cloze tests from text")
        p.add_argument("--text", required=True, help="Input passage")
        p.add_argument("--num_blanks", type=int, default=5)
        p.add_argument("--difficulty", choices=["easy", "medium", "hard"], default="medium")
        p.add_argument("--pos_targets", nargs="*", default=["nouns", "verbs", "adjectives"], help="Parts of speech to blank")
        p.add_argument("--no_ner", action="store_true", help="Allow blanking named entities")
        p.add_argument("--export_json", type=Path, help="Save quiz + answers to JSON file")
        return p
